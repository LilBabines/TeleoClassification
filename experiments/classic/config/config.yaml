hydra:
  run:
    dir: outputs/TeleoSplitGenera_300_medium_DNABERT-2-117_multiTaxa

data:
  dataset_path: "data/TeleoSplitGenera_300_medium/"

task:
  train : True  # False if just want to load model and evaluate, true to train before it
  checkpoint_path : "./outputs/TeleoSplitGenera_300_medium/DNABERT-2-117/multiTaxa/checkpoint-120000"
  save_preds : True
  task: "multiTaxa" # multiTaxa or singleTaxa
  
  # taxa_classes : ["order", "family"]

trainer:
  learning_rate: 1e-5
  per_device_train_batch_size : 16
  per_device_eval_batch_size : 16
  weight_decay: 0.01
  num_train_epochs : 30
  eval_delay : 0
  eval_strategy : "steps"
  eval_steps : 500
  logging_first_step : True
  save_strategy : "steps"
  
  metric_for_best_model: "macro_accuracy_family"
  greater_is_better: True
  push_to_hub : False
  # save best and last checkpoint only
  save_total_limit : 1
  load_best_model_at_end : True
  

model:
  model_name: "zhihan1996/DNABERT-2-117M"
  tokenizer_name: "zhihan1996/DNABERT-2-117M"  # can train "bpe" or an int for k-mer
  local : False  # set to True to load the model with model_name from local path
  bert_kwargs :
    hidden_size: 768
    max_position_embeddings: 514

loss :
  loss : "cross_entropy" # cross_entropy  ##### or BCEWithLogitsLoss not yet implemented


metrics:

  # macro_f1:
  #   callable: 'MulticlassF1Score'
  #   kwargs: 
  #     average: 'macro'
  macro_accuracy:
    callable: 'MulticlassAccuracy'
    kwargs:
      average: 'macro'
  # micro_f1:
  #   callable: 'MulticlassF1Score'
  #   kwargs: 
  #     average: 'micro'
  micro_accuracy:
    callable: 'MulticlassAccuracy'
    kwargs:
      average: 'micro'



  
